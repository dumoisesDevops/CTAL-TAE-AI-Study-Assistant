# ğŸ¤– CTAL-TAE AI Tutor

Agente de IA para auxiliar nos estudos da certificaÃ§Ã£o **ISTQB CTAL-TAE (Test Automation Engineer)**.  
O sistema gera simulados automÃ¡ticos, avalia respostas, explica conceitos e acompanha seu progresso.

---

## ğŸ¯ Objetivo do Projeto

Este projeto foi criado para automatizar o estudo e prÃ¡tica da certificaÃ§Ã£o **ISTQB CTAL-TAE**, permitindo:

- ğŸ§  GeraÃ§Ã£o automÃ¡tica de simulados com base no syllabus oficial  
- âœ… CorreÃ§Ã£o automÃ¡tica e explicaÃ§Ãµes detalhadas  
- ğŸ“Š Acompanhamento de desempenho e progresso  
- ğŸ“š RevisÃ£o personalizada por tÃ³pico  
- ğŸ”— IntegraÃ§Ã£o com modelos de linguagem (LLM) **open source e locais**

---

## âš™ï¸ Arquitetura do Sistema

```plaintext
Frontend (React + TypeScript + Vite)
â”‚   â”œâ”€â”€ Simulados, flashcards e estatÃ­sticas
â”‚
Backend (Python + FastAPI)
â”‚   â”œâ”€â”€ API REST para geraÃ§Ã£o, correÃ§Ã£o e progresso
â”‚
LLM Provider (Ollama + Llama3)
â”‚   â”œâ”€â”€ GeraÃ§Ã£o de perguntas, feedback e explicaÃ§Ãµes
â”‚
Banco de Dados (PostgreSQL)
â”‚   â”œâ”€â”€ Armazena usuÃ¡rios, simulados e desempenho
â”‚
Vector DB (Weaviate)
â”‚   â”œâ”€â”€ Busca semÃ¢ntica no syllabus
â”‚
Fila (Redis + Celery)
â”‚   â”œâ”€â”€ Processa geraÃ§Ã£o assÃ­ncrona de simulados
â”‚
CI/CD (GitHub Actions)
â”‚   â””â”€â”€ Build + Test + Deploy automÃ¡tico via Docker




| Componente         | Tecnologia                  | DescriÃ§Ã£o                 |
| ------------------ | --------------------------- | ------------------------- |
| **Backend**        | Python 3.11 + FastAPI       | API e orquestraÃ§Ã£o da IA  |
| **Frontend**       | React + Vite + Tailwind     | Interface web moderna     |
| **LLM Provider**   | Ollama (Llama3)             | Modelo local open source  |
| **Banco de Dados** | PostgreSQL                  | Armazenamento persistente |
| **Vector DB**      | Weaviate                    | Busca semÃ¢ntica           |
| **Cache / Fila**   | Redis + Celery              | Processos assÃ­ncronos     |
| **Infraestrutura** | Docker + GitHub Actions     | Deploy contÃ­nuo           |
| **Monitoramento**  | Grafana + Sentry (opcional) | Logs e mÃ©tricas           |


ğŸš€ Como ComeÃ§ar (Setup Local)
1ï¸âƒ£ Clonar o repositÃ³rio

git clone https://github.com/eduardomoises/ctal-tae-ai-tutor.git
cd ctal-tae-ai-tutor


2ï¸âƒ£ Criar arquivo .env na raiz

DATABASE_URL=postgresql://user:password@db:5432/ai_tutor
REDIS_URL=redis://redis:6379
PROVIDER=ollama
LLM_MODEL=llama3


**3ï¸âƒ£ Instalar o LLM local (sem custo)

Baixe o Ollama â†’ https://ollama.com/download**


**ollama pull llama3
ollama list
*4ï¸âƒ£ Rodar Backend (modo local)

cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

Acesse:
ğŸ‘‰ http://localhost:8000/docs

5ï¸âƒ£ Rodar Frontend (modo local)
cd ../frontend
npm install
npm run dev


Acesse:
ğŸ‘‰ http://localhost:5173

ğŸ³ Executando com Docker
1ï¸âƒ£ Subir o ambiente completo
docker-compose up --build

2ï¸âƒ£ Containers disponÃ­veis
ServiÃ§o	FunÃ§Ã£o
backend	API FastAPI
frontend	App React
db	PostgreSQL
redis	Fila de tarefas
weaviate	Banco vetorial
ollama	Modelo Llama3 local
ğŸ§  LLM Provider (Ollama)

O Ollama permite usar modelos locais de forma 100% gratuita.
Modelos recomendados:

ollama pull llama3
ollama pull mistral
ollama pull phi3


No .env, basta alternar:

LLM_MODEL=mistral

ğŸ”„ CI/CD â€“ GitHub Actions

Arquivo: .github/workflows/ci-cd.yml

name: CI/CD
on: [push]
jobs:
  build-test-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install deps
        run: pip install -r backend/requirements.txt
      - name: Run tests
        run: pytest backend/tests
      - name: Build Docker
        run: docker-compose build
      - name: Deploy
        run: echo "ğŸš€ (Deploy step - configure Render ou Vercel)"

ğŸ“Š Observabilidade (opcional)
Ferramenta	FunÃ§Ã£o
Sentry	Rastrear exceÃ§Ãµes do backend
Prometheus + Grafana	MÃ©tricas e monitoramento
Loki	Logs estruturados
ğŸ§  Como Estudar com o Agente

Suba o projeto (docker-compose up)

Acesse http://localhost:5173

Escolha um capÃ­tulo do syllabus

Clique em Gerar Simulado

Responda â†’ IA corrige e explica

Acompanhe seu progresso no dashboard

ğŸª„ PrÃ³ximas Etapas

ğŸ’¬ Chat com o syllabus (PDF + embeddings)

ğŸ§© RevisÃ£o automÃ¡tica de tÃ³picos fracos

ğŸ† Ranking e gamificaÃ§Ã£o

ğŸ¤– IntegraÃ§Ã£o com Telegram Bot

ğŸ§° ContribuiÃ§Ã£o
git checkout -b feature/nova-funcionalidade
git commit -m "add: nova feature"
git push origin feature/nova-funcionalidade


Abra um Pull Request para revisÃ£o e integraÃ§Ã£o.

ğŸ‘¨â€ğŸ’» Autor

Eduardo Moises
QA Engineer | Test Automation | Cloud & DevOps
ğŸ“§ contato: [seu-email]

ğŸ§¾ LicenÃ§a

Este projeto Ã© distribuÃ­do sob a licenÃ§a MIT â€“ uso livre e open source.
ğŸ“„ Consulte o arquivo LICENSE para mais detalhes.

ğŸ·ï¸ Badges





